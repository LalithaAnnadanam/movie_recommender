# -*- coding: utf-8 -*-
"""movierecengine.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1xbkgiptZhJ17l8kDZS4tnZftWEbFxgjg
"""

!pip install surprise

import zipfile
with zipfile.ZipFile('datasets.zip', 'r') as zip_ref:
    zip_ref.extractall('dataset')

import pandas as pd
import numpy as np
movies = pd.read_csv('/content/dataset/datasets/tmdb_5000_movies.csv')
df = pd.read_csv('/content/dataset/datasets/tmdb_5000_credits.csv')
df.columns = ['id', 'tittle', 'cast', 'crew']
movies = movies.merge(df, on='id')
movies.info()

movies.head(20)

C= movies['vote_average'].mean()
C

m= movies['vote_count'].quantile(0.9)
m

q_movies = movies.copy().loc[movies['vote_count'] >= m]
q_movies.shape

def weighted_rating(x, m=m, C=C):
    v = x['vote_count']
    R = x['vote_average']
    # Calculation based on the IMDB formula
    return (v/(v+m) * R) + (m/(m+v) * C)

q_movies['score'] = q_movies.apply(weighted_rating, axis=1)

#Sort movies based on score calculated above
q_movies = q_movies.sort_values('score', ascending=False)

#Print the top 15 movies
q_movies[['title', 'vote_count', 'vote_average', 'score']].head(15)

pop= movies.sort_values('popularity', ascending=False)
import matplotlib.pyplot as plt
plt.figure(figsize=(12,4))

plt.barh(pop['title'].head(6),pop['popularity'].head(6), align='center',
        color='skyblue')
plt.gca().invert_yaxis()
plt.xlabel("Popularity")
plt.title("Popular Movies")

pop[['title','popularity']].head(10)

from sklearn.feature_extraction.text import TfidfVectorizer
# removing english stop word like a, and , the 
tfidf = TfidfVectorizer(analyzer = 'word',stop_words = 'english')
#NaN -> ‘’
movies['overview'] = movies['overview'].fillna('')
tfidf_matrix = tfidf.fit_transform(movies['overview'])
tfidf_matrix.shape # outputs: (4803, 20978)

from sklearn.metrics.pairwise import linear_kernel
cosin_sim = linear_kernel(tfidf_matrix, tfidf_matrix)

index_of_movies = pd.Series(movies.index,   index=movies['title']).drop_duplicates()

def get_recommendations(title, cosin_sim=cosin_sim):
    idx = index_of_movies[title]
    
    sim_scores = list(enumerate(cosin_sim[idx]))
    # sorting of moviesidx based on similarity score
    sim_scores = sorted(sim_scores, key = lambda x:x[1], reverse = True)
    # get top 10 of sorted 
    sim_scores = sim_scores[1:31]
    
    movies_idx = [i[0] for i in sim_scores]
    
    return movies['title'].iloc[movies_idx]

get_recommendations('The Godfather').head(10)

from ast import literal_eval
features = ['cast', 'crew', 'keywords', 'genres']
for f in features:
    movies[f] = movies[f].apply(literal_eval)
# to get director from job
def get_director(x):
    for i in x:
        if i['job'] == 'Director':
            return i['name']
    return np.nan
# get top 3 elements of list
def get_list(x):
    if isinstance(x, list):
        names = [ i['name'] for i in x]
        
        if len(names)  > 3:
            names = names[:3]
        return names
    return []
#apply all functions
movies['director'] = movies['crew'].apply(get_director)
features = ['cast', 'keywords', 'genres']
for f in features:
    movies[f] = movies[f].apply(get_list)
#striping
def clean_data(x):
    if isinstance(x, list):
        return [str.lower(i.replace(' ', '')) for i in x]
    else:
        if isinstance(x, str):
            return str.lower(x.replace(' ', ''))
        else:
            return ''
features = ['cast', 'keywords', 'director', 'genres']
for f in features:
    movies[f] = movies[f].apply(clean_data)
#creating a SOUP
def create_soup(x):
    return ' '.join(x['keywords'])+' '+' '.join(x['cast'])+' '+x['director']+' '+' '.join(x['genres'])
movies['soup'] = movies.apply(create_soup, axis=1)
#count Vectorizer
from sklearn.feature_extraction.text import CountVectorizer
count = CountVectorizer(stop_words = 'english')
count_matrix = count.fit_transform(movies['soup'])
# finding similarity matrix
from sklearn.metrics.pairwise import cosine_similarity
cosin_sim2 = cosine_similarity(count_matrix, count_matrix)

cosin_sim2[7]

def get_recommendations(title, cosin_sim=cosin_sim):
    idx = index_of_movies[title]
    
    sim_scores = list(enumerate(cosin_sim[idx]))
    # sorting of moviesidx based on similarity score
    sim_scores = sorted(sim_scores, key = lambda x:x[1], reverse = True)
    # get top 10 of sorted 
    sim_scores = sim_scores[1:31]
    
    movies_idx = [i[0] for i in sim_scores]
    
    return movies['title'].iloc[movies_idx]

def recommend(movie):
  movie_index = movies[movies['title'] == movie].index[0]
  distances = cosin_sim2[movie_index]
  movies_list=sorted(list(enumerate(distances)),reverse=True,key=lambda x:x[1])[1:11]

  for i in movies_list:
    print(movies.iloc[i[0]].title)

recommend('The Avengers')

recommend('Titanic')

get_recommendations('The Avengers',cosin_sim2).head(10)

from surprise import Reader, Dataset, SVD
from surprise.model_selection import cross_validate
reader = Reader()
ratings = pd.read_csv('/content/dataset/datasets/ratings_small.csv')

SVD

data=Dataset.load_from_df(ratings[['userId','movieId','rating']], reader)
svd=SVD()
#Run 5-fold cross validation and print results
cross_validate(svd,data,measures=['RMSE'],cv=5,verbose=True)

train = data.build_full_trainset()
svd.fit(train)

svd.predict(23, 7)

# convert float val to int
def conv_int(x):
    try:
        return int(x)
    except:
        return np.nan

movie_id = pd.read_csv('/content/dataset/datasets/links.csv')[['movieId', 'tmdbId']]
movie_id['tmdbId'] = movie_id['tmdbId'].apply(conv_int)
movie_id.columns = ['movieId', 'id']
movie_id = movie_id.merge(movies[['title', 'id']], on='id').set_index('title')
print(movie_id.shape) # o/p: (4599, 2)
movie_id

index_map = movie_id.set_index('id')

def recommend_for(userId, title):
    index = index_of_movies[title]
    tmdbId = movie_id.loc[title]['id']
      

    #content based
    sim_scores = list(enumerate(cosin_sim2[int(index)]))
    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)
    sim_scores = sim_scores[11:21]
    movie_indices = [i[0] for i in sim_scores]

    mv = movies.iloc[movie_indices][['title', 'vote_count', 'vote_average', 'id']]
    mv = mv[mv['id'].isin(movie_id['id'])]
    print(mv)
    # CF
    mv['est'] = mv['id'].apply(lambda x: svd.predict(userId, index_map.loc[x]['movieId']).est)
    mv = mv.sort_values('est', ascending=False)
    return mv.head(10)

def recommend_for(userId, title):
    index = index_of_movies[title]
    tmdbId = movie_id.loc[title]['id']
      

    #content based
    sim_scores = list(enumerate(cosin_sim2[int(index)]))
    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)
    sim_scores = sim_scores[1:11]
    movie_indices = [i[0] for i in sim_scores]

    mv = movies.iloc[movie_indices][['title', 'vote_count', 'vote_average', 'id']]
    mv = mv[mv['id'].isin(movie_id['id'])]
    #movieslist = mv['title']
    #print(movieslist)
    print(mv)
    # CF
    mv['est'] = mv['id'].apply(lambda x: svd.predict(userId, index_map.loc[x]['movieId']).est)
    mv = mv.sort_values('est', ascending=False)
    return mv.head(10)

recommend_for(624,'Avatar')

recommend_for(4,'Titanic')

import pickle

movies.to_dict()

pickle.dump(movies.to_dict(),open('movies_dict.pkl','wb'))

pickle.dump(movie_id,open('moviesid.pkl','wb'))

pickle.dump(svd,open('svd.pkl','wb'))

pickle.dump(movies, open('model.pkl','wb'))

pickle.dump(cosin_sim2 , open('similarity.pkl','wb'))

import bz2
import pickle
import _pickle as cPickle

def compressed_pickle(title, data):
  with bz2.BZ2File(title + '.pbz2' , 'w') as f: 
    cPickle.dump(data, f)

compressed_pickle('Similarity', cosin_sim2)

compressed_pickle('movies_dict', movies.to_dict())

pickle.dump(movies , open('movie.pkl','wb'))